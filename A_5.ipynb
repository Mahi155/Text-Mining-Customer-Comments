{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A-5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1hDk9hDvOH7",
        "colab_type": "code",
        "outputId": "53153228-f01d-4fdf-cfa0-1ea98cac29c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#NLTK-------------------------------\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "#from nltk.stemporter import PorterStemmer\n",
        "# Import libraries for feature \n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#Change current working directory to gdrive\n",
        "%cd /gdrive\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMwGQK7KAd7T",
        "colab_type": "code",
        "outputId": "dba67568-8489-4418-e51b-251a63d12421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#Read files\n",
        "textfile = r'/gdrive/My Drive/508-A5/Comments.csv'\n",
        "textData = pd.read_csv(textfile) #creates a dataframe\n",
        "\n",
        "CustInfofile = r'/gdrive/My Drive/508-A5/Customers.csv'\n",
        "CustInfoData = pd.read_csv(CustInfofile)  #creates a dataframe\n",
        "\n",
        "print(textData.shape)\n",
        "print(CustInfoData.shape)\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 2)\n",
            "(2070, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWOTk6C1Ao45",
        "colab_type": "code",
        "outputId": "573f394f-2834-446b-bea6-e64a7937bd63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "#Extract target column from Customer Info file\n",
        "y_train = CustInfoData[\"TARGET\"]\n",
        "X_train = CustInfoData.drop(columns=[\"TARGET\"]) #extracting training data without the target column\n",
        "                     \n",
        "print(X_train.shape)\n",
        "print(textData.shape)\n",
        "textData.head()\n",
        "print(y_train)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 16)\n",
            "(2070, 2)\n",
            "0       Cancelled\n",
            "1         Current\n",
            "2         Current\n",
            "3         Current\n",
            "4       Cancelled\n",
            "          ...    \n",
            "2065    Cancelled\n",
            "2066    Cancelled\n",
            "2067    Cancelled\n",
            "2068    Cancelled\n",
            "2069    Cancelled\n",
            "Name: TARGET, Length: 2070, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq-IMi-EcyM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenize - Split the sentences to lists of words\n",
        "textData['CommentsTokenized'] = textData['Comments'].apply(word_tokenize)\n",
        "\n",
        "export_csv = textData.to_csv(r'/gdrive/My Drive/508-A5/TextDataTokenized1.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeY_VYnEfYwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use English stemmer.\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#Now do stemming - create a new dataframe to store stemmed version\n",
        "newTextData=pd.DataFrame()\n",
        "newTextData=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
        "newTextData['CommentsTokenizedStemmed'] = textData['CommentsTokenized'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
        "\n",
        "#export_csv = newTextData.to_csv(r'/gdrive/My Drive/508-A5/newTextDataTS-PP.csv')\n",
        "\n",
        "#Join stemmed strings\n",
        "newTextData['CommentsTokenizedStemmed'] = newTextData['CommentsTokenizedStemmed'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "#export_csv = newTextData.to_csv(r'/gdrive/My Drive/508-A5/newTextData-Joined-PP.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWr1s8CIgQmW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "784632c7-7117-4529-d96a-ee26c2c2f127"
      },
      "source": [
        "#Do Bag-Of-Words model - Term - Document Matrix\n",
        "#Learn the vocabulary dictionary and return term-document matrix.\n",
        "#count_vect = CountVectorizer(stop_words=None)\n",
        "count_vect = CountVectorizer(stop_words='english',lowercase=False)\n",
        "TD_counts = count_vect.fit_transform(newTextData.CommentsTokenizedStemmed)\n",
        "print(TD_counts.shape)\n",
        "TD_counts.dtype\n",
        "print(count_vect.get_feature_names())\n",
        "#print(TD_counts)\n",
        "DF_TD_Counts=pd.DataFrame(TD_counts.toarray())\n",
        "#print(DF_TD_Counts)\n",
        "#export_csv = DF_TD_Counts.to_csv(r'/gdrive/My Drive/508-A5/TD_counts-TokenizedStemmed.csv')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 366)\n",
            "['3399', '3g', 'As', 'CC', 'He', 'If', 'In', 'Is', 'It', 'We', 'abysm', 'access', 'accessori', 'adapt', 'add', 'addit', 'additon', 'address', 'adit', 'adress', 'advertis', 'afraid', 'alway', 'angel', 'angri', 'ani', 'anoth', 'anyth', 'anytim', 'area', 'asap', 'ask', 'bad', 'basic', 'bateri', 'batteri', 'becaus', 'believ', 'better', 'bigger', 'book', 'bought', 'brain', 'bring', 'built', 'busi', 'button', 'buy', 'cancel', 'cancer', 'car', 'care', 'carrier', 'caus', 'cc', 'cell', 'certain', 'chang', 'charg', 'charger', 'check', 'chip', 'citi', 'claim', 'cleariti', 'cold', 'comapr', 'compani', 'compar', 'competit', 'complain', 'complaint', 'concept', 'connect', 'consisit', 'consist', 'constanli', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cover', 'coverag', 'creat', 'credit', 'cstmer', 'cstmr', 'current', 'cust', 'custom', 'customr', 'date', 'day', 'dead', 'decent', 'defect', 'deo', 'did', 'die', 'differ', 'difficult', 'digiti', 'direct', 'disabl', 'doe', 'don', 'dont', 'drop', 'dure', 'easier', 'effect', 'encount', 'end', 'enemi', 'equip', 'everytim', 'everywher', 'evrey', 'exactli', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famili', 'featur', 'fed', 'figur', 'fine', 'fix', 'forev', 'forward', 'friend', 'function', 'furthermor', 'futur', 'gave', 'goat', 'good', 'great', 'gsm', 'ha', 'handset', 'happi', 'hard', 'hardli', 'hate', 'hear', 'heard', 'help', 'hi', 'higher', 'highway', 'hochi', 'hole', 'home', 'hope', 'horribl', 'hous', 'implement', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'june', 'just', 'kid', 'kno', 'know', 'lame', 'later', 'lctn', 'learn', 'leroy', 'like', 'line', 'list', 'lo', 'local', 'locat', 'locatn', 'long', 'lost', 'lot', 'love', 'major', 'make', 'manag', 'mani', 'manual', 'market', 'mean', 'messag', 'metropolitian', 'minut', 'misl', 'mistak', 'model', 'momma', 'mr', 'napeleon', 'near', 'nearest', 'need', 'network', 'new', 'news', 'notic', 'number', 'numer', 'offer', 'old', 'om', 'open', 'option', 'ori', 'ot', 'outbound', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phone', 'piec', 'plan', 'pleas', 'point', 'polici', 'poor', 'possibl', 'probabl', 'problem', 'properli', 'provid', 'provis', 'purpos', 'rate', 'rater', 'realiz', 'realli', 'reason', 'receiv', 'recept', 'recption', 'reenter', 'refer', 'relat', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'roll', 'rubbish', 'rude', 'said', 'sale', 'say', 'screen', 'self', 'send', 'servic', 'shitti', 'shut', 'sign', 'signal', 'significantli', 'simm', 'simpli', 'sinc', 'site', 'slow', 'sold', 'someon', 'sometim', 'soon', 'speak', 'speed', 'start', 'static', 'stole', 'store', 'stuff', 'stupid', 'substant', 'subtract', 'suck', 'suggest', 'supervisor', 'support', 'sure', 'surpris', 'suspect', 'suspend', 'switch', 'teach', 'technic', 'tell', 'terribl', 'test', 'text', 'thi', 'think', 'thought', 'ticket', 'till', 'time', 'tire', 'today', 'toilet', 'told', 'tone', 'tower', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'turn', 'uncomfort', 'understand', 'unhappi', 'unlimit', 'unreli', 'unwil', 'upset', 'usag', 'use', 'useless', 'valu', 'veri', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'whi', 'wife', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'york']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO9lVH2agRnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use English stemmer.\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "#Now do stemming - create a new dataframe to store stemmed version\n",
        "newTextData=pd.DataFrame()\n",
        "newTextData=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
        "newTextData['CommentsTokenizedStemmed'] = textData['CommentsTokenized'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
        "\n",
        "#export_csv = newTextData.to_csv(r'/gdrive/My Drive/508-A5/newTextDataTS-PP.csv')\n",
        "\n",
        "#Join stemmed strings\n",
        "newTextData['CommentsTokenizedStemmed'] = newTextData['CommentsTokenizedStemmed'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "#export_csv = newTextData.to_csv(r'/gdrive/My Drive/508-A5/newTextData-Joined-PP.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkdJH0eEgRhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "ed27de27-a4de-44ad-fc46-7bd67458e30a"
      },
      "source": [
        "#Do Bag-Of-Words model - Term - Document Matrix\n",
        "#Learn the vocabulary dictionary and return term-document matrix.\n",
        "#count_vect = CountVectorizer(stop_words=None)\n",
        "count_vect = CountVectorizer(stop_words='english',lowercase=False)\n",
        "TD_counts = count_vect.fit_transform(newTextData.CommentsTokenizedStemmed)\n",
        "print(TD_counts.shape)\n",
        "TD_counts.dtype\n",
        "print(count_vect.get_feature_names())\n",
        "#print(TD_counts)\n",
        "DF_TD_Counts=pd.DataFrame(TD_counts.toarray())\n",
        "#print(DF_TD_Counts)\n",
        "#export_csv = DF_TD_Counts.to_csv(r'/gdrive/My Drive/508-A5/TD_counts-TokenizedStemmed.csv')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 364)\n",
            "['3399', '3g', 'abysm', 'access', 'ad', 'adapt', 'addit', 'additon', 'address', 'adit', 'adress', 'advert', 'afraid', 'aft', 'al', 'alway', 'angel', 'angry', 'anoth', 'anyth', 'anytim', 'ar', 'asap', 'ask', 'bad', 'bas', 'batery', 'battery', 'becaus', 'believ', 'bet', 'big', 'bil', 'book', 'bought', 'brain', 'bring', 'built', 'busy', 'button', 'buy', 'cal', 'cancel', 'car', 'carry', 'caus', 'cc', 'cel', 'certain', 'chang', 'charg', 'check', 'chip', 'city', 'claim', 'clear', 'cold', 'comapr', 'comp', 'company', 'competit', 'complain', 'complaint', 'conceiv', 'connect', 'consisit', 'consist', 'const', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cov', 'cre', 'credit', 'cstmer', 'cstmr', 'cur', 'cust', 'custom', 'customr', 'dat', 'day', 'dead', 'dec', 'defect', 'deo', 'did', 'die', 'diff', 'difficult', 'digit', 'direct', 'dis', 'doe', 'don', 'dont', 'drop', 'dur', 'dying', 'easy', 'effect', 'encount', 'end', 'enemy', 'equip', 'ev', 'everytim', 'everywh', 'evrey', 'exact', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famy', 'feat', 'fed', 'fig', 'fin', 'fix', 'forev', 'forward', 'friend', 'funct', 'furtherm', 'fut', 'gav', 'giv', 'goat', 'going', 'good', 'gre', 'gsm', 'handset', 'happy', 'hard', 'hat', 'hav', 'hear', 'heard', 'help', 'high', 'highway', 'hochy', 'hol', 'hom', 'hop', 'horr', 'hous', 'impl', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'jun', 'just', 'kid', 'kno', 'know', 'lam', 'lat', 'lctn', 'learn', 'leroy', 'lik', 'lin', 'list', 'loc', 'locatn', 'long', 'los', 'lost', 'lot', 'lov', 'mad', 'maj', 'mak', 'man', 'market', 'mean', 'mess', 'metropolit', 'minut', 'misl', 'mistak', 'model', 'momm', 'mor', 'mov', 'mr', 'napeleon', 'near', 'nearest', 'nee', 'network', 'nev', 'new', 'num', 'numb', 'oft', 'old', 'omer', 'op', 'opt', 'ory', 'ot', 'oth', 'outbound', 'ov', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phon', 'piec', 'plan', 'pleas', 'point', 'policy', 'poor', 'poss', 'prob', 'problem', 'prop', 'provid', 'purpos', 'rat', 'real', 'reason', 'receiv', 'recpt', 'reent', 'refer', 'rel', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'rol', 'rub', 'rud', 'said', 'sal', 'sam', 'say', 'screening', 'self', 'send', 'serv', 'shitty', 'shut', 'sign', 'sim', 'simply', 'sint', 'sit', 'slow', 'sold', 'som', 'someon', 'sometim', 'soon', 'speak', 'spee', 'start', 'stat', 'stil', 'stol', 'stor', 'stuff', 'stupid', 'subst', 'subtract', 'suck', 'suggest', 'superv', 'support', 'sur', 'surpr', 'suspect', 'suspend', 'switch', 'tak', 'teach', 'techn', 'tel', 'terr', 'test', 'text', 'ther', 'thes', 'thi', 'think', 'thos', 'thought', 'ticket', 'til', 'tim', 'tir', 'today', 'toilet', 'told', 'ton', 'tow', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'try', 'turn', 'uncomfort', 'understand', 'unhappy', 'unlimit', 'unrely', 'unwil', 'upset', 'useless', 'valu', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'wel', 'wer', 'wher', 'wheth', 'whol', 'wif', 'wil', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'yo', 'york']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct7ZxdpFhFaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use English stemmer.\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "#Now do stemming - create a new dataframe to store stemmed version\n",
        "newTextData=pd.DataFrame()\n",
        "newTextData=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
        "newTextData['CommentsTokenizedStemmed'] = textData['CommentsTokenized'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
        "\n",
        "export_csv = newTextData.to_csv(r'/gdrive/My Drive/508-A5/newTextDataTS.csv')\n",
        "\n",
        "#Join stemmed strings\n",
        "newTextData['CommentsTokenizedStemmed'] = newTextData['CommentsTokenizedStemmed'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "export_csv = newTextData.to_csv(r'/gdrive/My Drive/508-A5/newTextData-Joined.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ci-dSuxhFRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "e3cbd991-7bbe-459d-edbf-663653d94a8f"
      },
      "source": [
        "#Do Bag-Of-Words model - Term - Document Matrix\n",
        "#Learn the vocabulary dictionary and return term-document matrix.\n",
        "#count_vect = CountVectorizer(stop_words=None)\n",
        "count_vect = CountVectorizer(stop_words='english',lowercase=False)\n",
        "TD_counts = count_vect.fit_transform(newTextData.CommentsTokenizedStemmed)\n",
        "print(TD_counts.shape)\n",
        "TD_counts.dtype\n",
        "print(count_vect.get_feature_names())\n",
        "#print(TD_counts)\n",
        "DF_TD_Counts=pd.DataFrame(TD_counts.toarray())\n",
        "#print(DF_TD_Counts)\n",
        "export_csv = DF_TD_Counts.to_csv(r'/gdrive/My Drive/508-A5/TD_counts-TokenizedStemmed.csv')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 354)\n",
            "['3399', '3g', 'abysm', 'access', 'accessori', 'adapt', 'add', 'addit', 'additon', 'address', 'adit', 'adress', 'advertis', 'afraid', 'alway', 'angel', 'angri', 'ani', 'anoth', 'anyth', 'anytim', 'area', 'asap', 'ask', 'bad', 'basic', 'bateri', 'batteri', 'becaus', 'believ', 'better', 'bigger', 'book', 'bought', 'brain', 'bring', 'built', 'busi', 'button', 'buy', 'cancel', 'cancer', 'car', 'care', 'carrier', 'caus', 'cc', 'cell', 'certain', 'chang', 'charg', 'charger', 'check', 'chip', 'citi', 'claim', 'cleariti', 'cold', 'comapr', 'compani', 'compar', 'competit', 'complain', 'complaint', 'concept', 'connect', 'consisit', 'consist', 'constan', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cover', 'coverag', 'creat', 'credit', 'cstmer', 'cstmr', 'current', 'cust', 'custom', 'customr', 'date', 'day', 'dead', 'decent', 'defect', 'deo', 'did', 'die', 'differ', 'difficult', 'digiti', 'direct', 'disabl', 'doe', 'don', 'dont', 'drop', 'dure', 'easier', 'effect', 'encount', 'end', 'enemi', 'equip', 'everytim', 'everywher', 'evrey', 'exact', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famili', 'featur', 'fed', 'figur', 'fine', 'fix', 'forev', 'forward', 'friend', 'function', 'furthermor', 'futur', 'gave', 'goat', 'good', 'great', 'gsm', 'handset', 'happi', 'hard', 'hate', 'hear', 'heard', 'help', 'higher', 'highway', 'hochi', 'hole', 'home', 'hope', 'horribl', 'hous', 'implement', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'june', 'just', 'kid', 'kno', 'know', 'lame', 'later', 'lctn', 'learn', 'leroy', 'like', 'line', 'list', 'local', 'locat', 'locatn', 'long', 'los', 'lost', 'lot', 'love', 'major', 'make', 'manag', 'mani', 'manual', 'market', 'mean', 'messag', 'metropolitian', 'minut', 'misl', 'mistak', 'model', 'momma', 'mr', 'napeleon', 'near', 'nearest', 'need', 'network', 'new', 'news', 'notic', 'number', 'numer', 'offer', 'old', 'om', 'open', 'option', 'ori', 'ot', 'outbound', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phone', 'piec', 'plan', 'pleas', 'point', 'polici', 'poor', 'possibl', 'probabl', 'problem', 'proper', 'provid', 'provis', 'purpos', 'rate', 'rater', 'realiz', 'realli', 'reason', 'receiv', 'recept', 'recption', 'reenter', 'refer', 'relat', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'roll', 'rubbish', 'rude', 'said', 'sale', 'say', 'screen', 'self', 'send', 'servic', 'shitti', 'shut', 'sign', 'signal', 'signific', 'simm', 'simpli', 'sinc', 'site', 'slow', 'sold', 'someon', 'sometim', 'soon', 'speak', 'speed', 'start', 'static', 'stole', 'store', 'stuff', 'stupid', 'substant', 'subtract', 'suck', 'suggest', 'supervisor', 'support', 'sure', 'surpris', 'suspect', 'suspend', 'switch', 'teach', 'technic', 'tell', 'terribl', 'test', 'text', 'think', 'thought', 'ticket', 'till', 'time', 'tire', 'today', 'toilet', 'told', 'tone', 'tower', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'turn', 'uncomfort', 'understand', 'unhappi', 'unlimit', 'unreli', 'unwil', 'upset', 'usag', 'use', 'useless', 'valu', 'veri', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'whi', 'wife', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'york']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd8TZYnAxQbP",
        "colab_type": "code",
        "outputId": "1160ec14-8dd4-4cc8-9e9a-f25346a58026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "#Compute TF-IDF Matrix\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(TD_counts)\n",
        "print(X_train_tfidf.shape)\n",
        "DF_TF_IDF=pd.DataFrame(X_train_tfidf.toarray())\n",
        "print(DF_TF_IDF)\n",
        "export_csv= DF_TF_IDF.to_csv(r'/gdrive/My Drive/508-A5/TFIDF_counts-TokenizedStemmed.csv')\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 354)\n",
            "      0    1    2    3    4    5    6    ...       347  348  349  350  351  352  353\n",
            "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...       ...  ...  ...  ...  ...  ...  ...\n",
            "2065  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.356121  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2066  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.356121  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2067  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.356121  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2068  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.356121  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2069  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.356121  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 354 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_HLkohqiVN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#List of Categorical Features\n",
        "categoricalFeatures = [\"ID\", \t'Sex', \t'Status',\t'Children',\t\"Est_Income\", \t\"Car_Owner\", \t\"Usage\", \t'Age', \t\"RatePlan\", \t\"LongDistance\", \t\n",
        "                       \"International\", \t\"Local\", \t\"Dropped\", \t\"Paymethod\", \t\"LocalBilltype\", \t\"LongDistanceBilltype\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6_4_QMliVFC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "11db1567-1924-4c7c-94f3-d65932f59598"
      },
      "source": [
        "#Combine Train and test for one Hot Encoding\n",
        "combined_Data = pd.concat([X_train, DF_TF_IDF], axis = 1)\n",
        "\n",
        "#Do one Hot encoding for categorical features for combined data\n",
        "combined_data = pd.get_dummies(combined_Data,columns=categoricalFeatures)\n",
        "\n",
        "print(\"combined_data\",combined_data.shape)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "combined_data (2070, 4136)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2owIUD6_eYO",
        "colab_type": "code",
        "outputId": "36e96f3f-e318-44b6-cdf5-b6286b19b099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "#Feature selection Filter\n",
        "new_DF_TF_IDF = SelectKBest(score_func=chi2, k=50).fit_transform(combined_data,y_train)\n",
        "new_DF_TF_IDF.shape\n",
        "\n",
        "DF_TF_IDF_SelectedFeatures= pd.DataFrame(new_DF_TF_IDF)\n",
        "print(DF_TF_IDF_SelectedFeatures)\n",
        "\n",
        "export_csv= DF_TF_IDF_SelectedFeatures.to_csv(r'/gdrive/My Drive/508-A5/TFIDF_counts-Selected Features.csv')\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       0         1    2    3    4        5   ...   44   45   46   47   48   49\n",
            "0     0.0  0.000000  0.0  0.0  0.0  0.00000  ...  0.0  0.0  0.0  0.0  0.0  1.0\n",
            "1     0.0  0.000000  0.0  0.0  0.0  0.37653  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2     0.0  0.000000  0.0  0.0  0.0  0.37653  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3     0.0  0.000000  0.0  0.0  0.0  0.37653  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4     0.0  0.000000  0.0  0.0  0.0  0.37653  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "...   ...       ...  ...  ...  ...      ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065  0.0  0.466708  0.0  0.0  0.0  0.00000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2066  0.0  0.466708  0.0  0.0  0.0  0.00000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2067  0.0  0.466708  0.0  0.0  0.0  0.00000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2068  0.0  0.466708  0.0  0.0  0.0  0.00000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2069  0.0  0.466708  0.0  0.0  0.0  0.00000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 50 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geVCLka8xxjf",
        "colab_type": "code",
        "outputId": "aa767212-7c07-4b12-f510-5ae309ce4887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "#Construct a Random Forest Classifier on text data\n",
        "clf=RandomForestClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_SelectedFeatures,y_train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_SelectedFeatures, y_train)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_SelectedFeatures)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_train, rf_predictions))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.828502\n",
            "Confusion Matrix:\n",
            "[[ 696  108]\n",
            " [ 247 1019]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.74      0.87      0.80       804\n",
            "     Current       0.90      0.80      0.85      1266\n",
            "\n",
            "    accuracy                           0.83      2070\n",
            "   macro avg       0.82      0.84      0.82      2070\n",
            "weighted avg       0.84      0.83      0.83      2070\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a-4yV_FuSLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "1a89765a-6cfb-483c-fa8d-5cfe8c22d73d"
      },
      "source": [
        "#Construct a Decision Tree Classifier on text data\n",
        "clf=DecisionTreeClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_SelectedFeatures,y_train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_SelectedFeatures, y_train)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_SelectedFeatures)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_train, rf_predictions))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.828986\n",
            "Confusion Matrix:\n",
            "[[ 697  107]\n",
            " [ 247 1019]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.74      0.87      0.80       804\n",
            "     Current       0.90      0.80      0.85      1266\n",
            "\n",
            "    accuracy                           0.83      2070\n",
            "   macro avg       0.82      0.84      0.82      2070\n",
            "weighted avg       0.84      0.83      0.83      2070\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq_7eeQOz7xk",
        "colab_type": "code",
        "outputId": "6ca98f9a-4ab5-4ed9-a744-1c2ed7f48b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "#Merge files\n",
        "\n",
        "print(CustInfoData.shape)\n",
        "X_train = CustInfoData.drop(columns=[\"TARGET\"]) #extracting training data without the target column\n",
        "print(X_train.shape)\n",
        "combined=pd.concat([X_train, DF_TF_IDF_SelectedFeatures], axis=1)\n",
        "print(combined.shape)\n",
        "print(combined)\n",
        "export_csv= combined.to_csv(r'/gdrive/My Drive/508-A5/Combined-Cust+TFIDF+SelectedFeatures.csv')\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 17)\n",
            "(2070, 16)\n",
            "(2070, 66)\n",
            "        ID Sex Status  Children  Est_Income  ...   45   46   47   48   49\n",
            "0        1   F      S         1    38000.00  ...  0.0  0.0  0.0  0.0  1.0\n",
            "1        6   M      M         2    29616.00  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2        8   M      M         0    19732.80  ...  0.0  0.0  0.0  0.0  0.0\n",
            "3       11   M      S         2       96.33  ...  0.0  0.0  0.0  0.0  0.0\n",
            "4       14   F      M         2    52004.80  ...  0.0  0.0  0.0  0.0  0.0\n",
            "...    ...  ..    ...       ...         ...  ...  ...  ...  ...  ...  ...\n",
            "2065  3821   F      S         0    78851.30  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2066  3822   F      S         1    17540.70  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2067  3823   F      M         0    83891.90  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2068  3824   F      M         2    28220.80  ...  0.0  0.0  0.0  0.0  0.0\n",
            "2069  3825   F      S         0    28589.10  ...  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 66 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntKijZPste0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "8c6ac7fb-cc2e-4f28-cbd7-d21a97855371"
      },
      "source": [
        "#Feature selection Filter\n",
        "new_DF_TF_IDF = SelectKBest(score_func=chi2, k=1000).fit_transform(combined_data,y_train)\n",
        "new_DF_TF_IDF.shape\n",
        "\n",
        "DF_TF_IDF_SelectedFeatures= pd.DataFrame(new_DF_TF_IDF)\n",
        "print(DF_TF_IDF_SelectedFeatures)\n",
        "\n",
        "export_csv= DF_TF_IDF_SelectedFeatures.to_csv(r'/gdrive/My Drive/508-A5/TFIDF_counts-Selected Features.csv')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      0    1    2    3         4         5    ...  994  995  996  997  998  999\n",
            "0     0.0  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  1.0  1.0  0.0\n",
            "1     0.0  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  1.0\n",
            "2     0.0  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  1.0  0.0  1.0\n",
            "3     0.0  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  1.0  0.0  1.0\n",
            "4     0.0  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  1.0  0.0\n",
            "...   ...  ...  ...  ...       ...       ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065  0.0  0.0  0.0  0.0  0.466708  0.443664  ...  0.0  0.0  0.0  1.0  0.0  1.0\n",
            "2066  0.0  0.0  0.0  0.0  0.466708  0.443664  ...  0.0  0.0  1.0  0.0  0.0  1.0\n",
            "2067  0.0  0.0  0.0  0.0  0.466708  0.443664  ...  0.0  0.0  0.0  0.0  0.0  1.0\n",
            "2068  0.0  0.0  0.0  0.0  0.466708  0.443664  ...  0.0  0.0  0.0  1.0  0.0  1.0\n",
            "2069  0.0  0.0  0.0  0.0  0.466708  0.443664  ...  0.0  0.0  0.0  1.0  0.0  1.0\n",
            "\n",
            "[2070 rows x 1000 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoWuTgA0tfEY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "8c9dfbb3-99e4-421c-a03b-47ebbb61cff8"
      },
      "source": [
        "#Construct a Random Forest Classifier on text data\n",
        "clf=RandomForestClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_SelectedFeatures,y_train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_SelectedFeatures, y_train)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_SelectedFeatures)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_train, rf_predictions))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.950725\n",
            "Confusion Matrix:\n",
            "[[ 757   47]\n",
            " [  55 1211]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.93      0.94      0.94       804\n",
            "     Current       0.96      0.96      0.96      1266\n",
            "\n",
            "    accuracy                           0.95      2070\n",
            "   macro avg       0.95      0.95      0.95      2070\n",
            "weighted avg       0.95      0.95      0.95      2070\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbOrtU0itfbK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "abd71314-058b-45d2-9be0-4819f6b6d16a"
      },
      "source": [
        "#Construct a Decision Tree Classifier on text data\n",
        "clf=DecisionTreeClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_SelectedFeatures,y_train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_SelectedFeatures, y_train)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_SelectedFeatures)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_train, rf_predictions))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.956522\n",
            "Confusion Matrix:\n",
            "[[ 782   22]\n",
            " [  68 1198]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.92      0.97      0.95       804\n",
            "     Current       0.98      0.95      0.96      1266\n",
            "\n",
            "    accuracy                           0.96      2070\n",
            "   macro avg       0.95      0.96      0.95      2070\n",
            "weighted avg       0.96      0.96      0.96      2070\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lGAl2MVtfqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "efecc638-1522-40ff-bb52-154329042b21"
      },
      "source": [
        "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
        "\n",
        "#Construct a Random Forest Classifier on text data\n",
        "clf=RandomForestClassifier()\n",
        "\n",
        "# Build step forward feature selection\n",
        "sfs1 = sfs(clf,\n",
        "           k_features=3,\n",
        "           forward=True,\n",
        "           floating=False,\n",
        "           verbose=2,\n",
        "           scoring='accuracy',\n",
        "           cv=3)\n",
        "\n",
        "RF_text = sfs1.fit(combined_data,y_train)\n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4136 out of 4136 | elapsed:  3.4min finished\n",
            "\n",
            "[2019-12-14 00:50:49] Features: 1/3 -- score: 0.6806763285024156[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4135 out of 4135 | elapsed:  3.8min finished\n",
            "\n",
            "[2019-12-14 00:54:34] Features: 2/3 -- score: 0.7410628019323672[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4134 out of 4134 | elapsed:  3.8min finished\n",
            "\n",
            "[2019-12-14 00:58:21] Features: 3/3 -- score: 0.7922705314009661"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ri1dzJRtfy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "e8dee5df-001e-42b5-c263-3efc6939d9af"
      },
      "source": [
        "#Construct a Random Forest Classifier on text data\n",
        "clf=RandomForestClassifier()\n",
        "\n",
        "# Build step forward feature selection\n",
        "sfs1 = sfs(clf,\n",
        "           k_features=5,\n",
        "           forward=True,\n",
        "           floating=False,\n",
        "           verbose=2,\n",
        "           scoring='accuracy',\n",
        "           cv=5)\n",
        "\n",
        "RF_text = sfs1.fit(combined_data,y_train)\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4136 out of 4136 | elapsed:  5.5min finished\n",
            "\n",
            "[2019-12-14 01:03:51] Features: 1/5 -- score: 0.6821003577773916[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4135 out of 4135 | elapsed:  6.1min finished\n",
            "\n",
            "[2019-12-14 01:09:57] Features: 2/5 -- score: 0.7434567155874788[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4134 out of 4134 | elapsed:  6.0min finished\n",
            "\n",
            "[2019-12-14 01:15:55] Features: 3/5 -- score: 0.7951499833773809[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4133 out of 4133 | elapsed:  6.2min finished\n",
            "\n",
            "[2019-12-14 01:22:07] Features: 4/5 -- score: 0.8323539672251838[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4132 out of 4132 | elapsed:  6.3min finished\n",
            "\n",
            "[2019-12-14 01:28:22] Features: 5/5 -- score: 0.8492761881649489"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bGPBt8atgPb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "e024e445-6755-44e6-b98d-ef05138cf084"
      },
      "source": [
        "#Construct a Decision Tree Classifier on text data\n",
        "clf=DecisionTreeClassifier()\n",
        "\n",
        "# Build step forward feature selection\n",
        "sfs1 = sfs(clf,\n",
        "           k_features=3,\n",
        "           forward=True,\n",
        "           floating=False,\n",
        "           verbose=2,\n",
        "           scoring='accuracy',\n",
        "           cv=3)\n",
        "\n",
        "RF_text = sfs1.fit(combined_data,y_train)\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4136 out of 4136 | elapsed:   49.5s finished\n",
            "\n",
            "[2019-12-14 01:29:12] Features: 1/3 -- score: 0.6801932367149758[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4135 out of 4135 | elapsed:   53.2s finished\n",
            "\n",
            "[2019-12-14 01:30:06] Features: 2/3 -- score: 0.7425120772946859[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4134 out of 4134 | elapsed:   53.8s finished\n",
            "\n",
            "[2019-12-14 01:31:00] Features: 3/3 -- score: 0.7903381642512078"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b247s0bftgbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "860727a4-1804-4930-bd8d-6cc384fd2666"
      },
      "source": [
        "#Construct a Decision Tree Classifier on text data\n",
        "clf=DecisionTreeClassifier()\n",
        "# Build step forward feature selection\n",
        "sfs1 = sfs(clf,\n",
        "           k_features=5,\n",
        "           forward=True,\n",
        "           floating=False,\n",
        "           verbose=2,\n",
        "           scoring='accuracy',\n",
        "           cv=5)\n",
        "RF_text = sfs1.fit(combined_data,y_train)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4136 out of 4136 | elapsed:  1.2min finished\n",
            "\n",
            "[2019-12-14 01:32:13] Features: 1/5 -- score: 0.6830688751426381[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4135 out of 4135 | elapsed:  1.3min finished\n",
            "\n",
            "[2019-12-14 01:33:33] Features: 2/5 -- score: 0.7429736238000393[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4134 out of 4134 | elapsed:  1.4min finished\n",
            "\n",
            "[2019-12-14 01:34:55] Features: 3/5 -- score: 0.7932187803042187[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4133 out of 4133 | elapsed:  1.4min finished\n",
            "\n",
            "[2019-12-14 01:36:19] Features: 4/5 -- score: 0.8347705902389781[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4132 out of 4132 | elapsed:  1.4min finished\n",
            "\n",
            "[2019-12-14 01:37:44] Features: 5/5 -- score: 0.8507278085919847"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvsbqkS3tfVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "0b00a531-3f75-4e8b-9c67-e5d967a5a9ec"
      },
      "source": [
        "#split the entire training data in training and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_Train, X_Test, Y_Train, Y_Test = train_test_split( combined_data, y_train, test_size=0.20, random_state=42)\n",
        "print(\"Initial shape for entire data:\",combined_data.shape)\n",
        "print(\"Shape of new training data:\", X_Train.shape)\n",
        "print(\"Shape of new test split data:\", X_Test.shape)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial shape for entire data: (2070, 4136)\n",
            "Shape of new training data: (1656, 4136)\n",
            "Shape of new test split data: (414, 4136)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddWNHwZaEm8-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "0c5fc53a-47d2-4558-efb1-10fb9f2cd412"
      },
      "source": [
        "#Feature selection Filter\n",
        "new_DF_TF_IDF10 = SelectKBest(score_func=chi2, k=1000).fit(X_Train,Y_Train)\n",
        "new_DF_TF_IDF = pd.DataFrame(new_DF_TF_IDF10.transform(X_Train))\n",
        "\n",
        "DF_TF_IDF_SelectedFeatures= pd.DataFrame(new_DF_TF_IDF)\n",
        "\n",
        "\n",
        "DF_TF_IDF_SelectedFeatures2 = pd.DataFrame(new_DF_TF_IDF10.transform(X_Test))\n",
        "\n",
        "print(DF_TF_IDF_SelectedFeatures2)\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          0    1         2    3         4    ...  995  996  997  998  999\n",
            "0    0.368149  0.0  0.334347  0.0  0.000000  ...  0.0  0.0  1.0  1.0  0.0\n",
            "1    0.000000  0.0  0.000000  0.0  0.000000  ...  0.0  0.0  1.0  0.0  1.0\n",
            "2    0.000000  0.0  0.000000  0.0  0.000000  ...  0.0  0.0  0.0  0.0  1.0\n",
            "3    0.000000  0.0  0.000000  0.0  0.000000  ...  0.0  0.0  0.0  0.0  1.0\n",
            "4    0.000000  0.0  0.000000  0.0  0.000000  ...  0.0  0.0  1.0  0.0  1.0\n",
            "..        ...  ...       ...  ...       ...  ...  ...  ...  ...  ...  ...\n",
            "409  0.368149  0.0  0.334347  0.0  0.000000  ...  0.0  0.0  1.0  0.0  1.0\n",
            "410  0.000000  0.0  0.000000  0.0  0.000000  ...  0.0  0.0  0.0  0.0  1.0\n",
            "411  0.000000  0.0  0.000000  0.0  0.000000  ...  0.0  0.0  0.0  0.0  1.0\n",
            "412  0.000000  0.0  0.000000  0.0  0.000000  ...  0.0  0.0  1.0  1.0  0.0\n",
            "413  0.000000  0.0  0.000000  0.0  0.466708  ...  0.0  1.0  0.0  0.0  1.0\n",
            "\n",
            "[414 rows x 1000 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0oYbYrNte-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "11a01fe1-656f-459a-e3e2-c419e2057e4c"
      },
      "source": [
        "#Construct a Decision Tree Classifier on text data\n",
        "clf=DecisionTreeClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_SelectedFeatures,Y_Train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_SelectedFeatures, Y_Train)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_SelectedFeatures2)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_Test, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(Y_Test, rf_predictions))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.963164\n",
            "Confusion Matrix:\n",
            "[[126  31]\n",
            " [ 32 225]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.80      0.80      0.80       157\n",
            "     Current       0.88      0.88      0.88       257\n",
            "\n",
            "    accuracy                           0.85       414\n",
            "   macro avg       0.84      0.84      0.84       414\n",
            "weighted avg       0.85      0.85      0.85       414\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HFKLJVrteuE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "f059a9f6-9d63-4e53-a897-382fbc68589a"
      },
      "source": [
        "#Construct a Decision Tree Classifier on text data\n",
        "clf=DecisionTreeClassifier()\n",
        "\n",
        "# Build step forward feature selection\n",
        "sfs10 = sfs(clf,\n",
        "           k_features=5,\n",
        "           forward=True,\n",
        "           floating=False,\n",
        "           verbose=2,\n",
        "           scoring='accuracy',\n",
        "           cv=5)\n",
        "\n",
        "RF_text = sfs10.fit(X_Train,Y_Train)\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4136 out of 4136 | elapsed:  1.1min finished\n",
            "\n",
            "[2019-12-14 02:11:33] Features: 1/5 -- score: 0.6745010142189191[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4135 out of 4135 | elapsed:  1.2min finished\n",
            "\n",
            "[2019-12-14 02:12:44] Features: 2/5 -- score: 0.7391009795843633[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4134 out of 4134 | elapsed:  1.2min finished\n",
            "\n",
            "[2019-12-14 02:13:54] Features: 3/5 -- score: 0.7831734959613239[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4133 out of 4133 | elapsed:  1.2min finished\n",
            "\n",
            "[2019-12-14 02:15:06] Features: 4/5 -- score: 0.8169721787451923[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 4132 out of 4132 | elapsed:  1.2min finished\n",
            "\n",
            "[2019-12-14 02:16:21] Features: 5/5 -- score: 0.8369118660856003"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzwGhXMMNEEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f19dd753-f8d2-4212-eda2-12957101b547"
      },
      "source": [
        "#selected features\n",
        "selected_features = list(sfs10.k_feature_names_)\n",
        "print(selected_features)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[199, 335, 'Sex_F', 'Children_2', 'Paymethod_Auto']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d54VN68qtelM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "837da7db-d99d-4512-cb93-268d18597f92"
      },
      "source": [
        "\n",
        "RF_text = clf.fit(X_Train[selected_features],Y_Train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_Train[selected_features], Y_Train)))\n",
        "rf_predictions = clf.predict(X_Test[selected_features])\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_Test, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(Y_Test, rf_predictions))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.846618\n",
            "Confusion Matrix:\n",
            "[[120  37]\n",
            " [ 32 225]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.79      0.76      0.78       157\n",
            "     Current       0.86      0.88      0.87       257\n",
            "\n",
            "    accuracy                           0.83       414\n",
            "   macro avg       0.82      0.82      0.82       414\n",
            "weighted avg       0.83      0.83      0.83       414\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}